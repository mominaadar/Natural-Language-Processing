{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('IMDB-Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc : row, col\n",
    "review_docs = raw_data.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = raw_data.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x17890 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5894889 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df = 20, max_df = 30000).fit_transform(review_docs)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size=0.8 : want to set 80% of data for training\n",
    "# shuffle=True : important when dataset has last instances as of same level i.e. when dataset is ordered, doesn't give good chance to verify performance of model, mixes instances\n",
    "training_docs, testing_docs, training_senti, testing_senti = train_test_split(tfidf, senti, train_size = 0.8, shuffle=True)\n",
    "\n",
    "# tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<40000x17890 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 4723243 stored elements in Compressed Sparse Row format>,\n",
       " 40000)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_docs, len(training_senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = MultinomialNB().fit(tfidf[:-10], senti[:-10])\n",
    "clf = MultinomialNB().fit(training_docs, training_senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.predict(tfidf[-10:])\n",
    "predicted_senti = clf.predict(testing_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(testing_senti, predicted_senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is: 0.8601\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score is:',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation, dataset to be split in 5 parts, process will repeat 5 times\n",
    "kf = KFold(n_splits = 5)\n",
    "\n",
    "# x = [1,2,3,4,5,6]\n",
    "# labels = [0,0,0,1,1,1]\n",
    "\n",
    "acc = []\n",
    "\n",
    "# loop will run 5 times\n",
    "for train_ids, test_ids in kf.split(tfidf, senti):\n",
    "    \n",
    "    train_docs, test_docs = tfidf[train_ids], tfidf[test_ids]\n",
    "    train_senti, test_senti = senti[train_ids], senti[test_ids]\n",
    "    \n",
    "    clf = MultinomialNB().fit(train_docs, train_senti)\n",
    "    \n",
    "    predicted_senti = clf.predict(test_docs)\n",
    "    \n",
    "    acc.append(accuracy_score(test_senti, predicted_senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8603\n"
     ]
    }
   ],
   "source": [
    "total_acc = sum(acc)/len(acc)\n",
    "print('Accuracy score:',total_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Leave-one-out Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ids: [1 2 3 4 5 6] test ids: [0]\n",
      "train ids: [0 2 3 4 5 6] test ids: [1]\n",
      "train ids: [0 1 3 4 5 6] test ids: [2]\n",
      "train ids: [0 1 2 4 5 6] test ids: [3]\n",
      "train ids: [0 1 2 3 5 6] test ids: [4]\n",
      "train ids: [0 1 2 3 4 6] test ids: [5]\n",
      "train ids: [0 1 2 3 4 5] test ids: [6]\n"
     ]
    }
   ],
   "source": [
    "# automatically does N split so n_splits is not set\n",
    "# we use when we don't have enough data to train the model\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "x = [1,2,3,4,5,6,7]\n",
    "labels = [0,0,0,1,1,1,1]\n",
    "\n",
    "acc = []\n",
    "\n",
    "# loop will run N times\n",
    "# will take too much time when run on 50k dataset\n",
    "for train_ids, test_ids in loo.split(x):\n",
    "    \n",
    "    print('train ids:',train_ids, 'test ids:',test_ids)\n",
    "    \n",
    "#     train_docs, test_docs = tfidf[train_ids], tfidf[test_ids]\n",
    "#     train_senti, test_senti = senti[train_ids], senti[test_ids]\n",
    "    \n",
    "#     clf = MultinomialNB().fit(train_docs, train_senti)\n",
    "    \n",
    "#     predicted_senti = clf.predict(test_docs)\n",
    "    \n",
    "#     acc.append(accuracy_score(test_senti, predicted_senti))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.4 Predictive Accuracy of KNN using KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('dataset3.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = corpus.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for i in docs_list:\n",
    "    if i != '':\n",
    "        docs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "\n",
    "for doc in docs:\n",
    "    i, l = doc.split(':')\n",
    "    x.append(i.strip())\n",
    "    y.append(l.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_x = vec.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# apply validation and get split\n",
    "score = 0\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for train_ids, test_ids in kf.split(matrix_x):\n",
    "    train_x, test_x = matrix_x[train_ids], matrix_x[test_ids]\n",
    "    train_y, test_y = y[train_ids], y[test_ids]\n",
    "    \n",
    "    knn.fit(train_x, train_y)\n",
    "    \n",
    "    predicted_y = knn.predict(test_x)\n",
    "    \n",
    "    score += accuracy_score(test_y, predicted_y, normalize=True)\n",
    "    \n",
    "print(score/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.5 Precision, Recall and F1-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8601\n",
      "Precision score: 0.8604386139744583\n",
      "Recall score: 0.8601\n",
      "F1 score: 0.8600624745840181\n"
     ]
    }
   ],
   "source": [
    "# switch off normalization for accuracy score\n",
    "# normalize=False : gives correct number of instances identifies\n",
    "# normalize=True : correct no. of instances / total no. of instances\n",
    "\n",
    "accuracy = accuracy_score(testing_senti, predicted_senti, normalize=True)\n",
    "print('Accuracy score:',accuracy)\n",
    "\n",
    "\n",
    "# average='macro' : calculate precision for each label separately and avg the two, use when almost equal no. of representation of all the labels in dataset\n",
    "# average='micro' : average the precision at the instance level for all labels\n",
    "# average='weighted' : weights are assigned based on the instances\n",
    "\n",
    "precision = precision_score(testing_senti, predicted_senti, average='weighted')\n",
    "print('Precision score:',precision)\n",
    "\n",
    "\n",
    "recall = recall_score(testing_senti, predicted_senti, average='weighted')\n",
    "print('Recall score:',recall)\n",
    "\n",
    "\n",
    "f1 = f1_score(testing_senti, predicted_senti, average='weighted')\n",
    "print('F1 score:',f1)\n",
    "\n",
    "# average scheme should be same for all the models so we can pick a model with better score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.6 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4387 correctly classified\n",
    "# 622 misclassfied as -ve instances, were actually +ve\n",
    "# 777 wrongly classified as +ve, were actually -ve\n",
    "# 4214 correctly classified\n",
    "\n",
    "cm = confusion_matrix(testing_senti, predicted_senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4387,  622],\n",
       "       [ 777, 4214]])"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.7 Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 4)\n",
    "nb = MultinomialNB()\n",
    "dt = DecisionTreeClassifier(max_depth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('dataset3.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = corpus.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for i in docs_list:\n",
    "    if i != '':\n",
    "        docs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "\n",
    "for doc in docs:\n",
    "    i, l = doc.split(':')\n",
    "    x.append(i.strip())\n",
    "    y.append(l.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_x = vec.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB score: 1.0\n",
      "DT score: 0.875\n"
     ]
    }
   ],
   "source": [
    "f1_nb_score = 0\n",
    "f1_dt_score = 0\n",
    "\n",
    "for train_ids, test_ids in kf.split(matrix_x):\n",
    "    train_x, test_x = matrix_x[train_ids], matrix_x[test_ids]\n",
    "    train_y, test_y = y[train_ids], y[test_ids]\n",
    "    \n",
    "    nb.fit(train_x, train_y)\n",
    "    dt.fit(train_x, train_y)\n",
    "    \n",
    "    predicted_nb_y = nb.predict(test_x)\n",
    "    predicted_dt_y = dt.predict(test_x)\n",
    "    \n",
    "    f1_nb_score += f1_score(test_y, predicted_nb_y, average='micro')\n",
    "    f1_dt_score += f1_score(test_y, predicted_dt_y, average='micro')\n",
    "    \n",
    "print('NB score:',f1_nb_score/4)\n",
    "print('DT score:',f1_dt_score/4)\n",
    "\n",
    "# NB has scored better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.8 Implementing Clustering Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('dataset2.csv', encoding='latin-1').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = corpus.split('\\n')\n",
    "docs.remove(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<652x2457 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 14788 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_x = vec.fit_transform(docs)\n",
    "matrix_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=2)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(matrix_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.538962591186229\n",
      "0.5333093440770192\n"
     ]
    }
   ],
   "source": [
    "# input data should be in array so matrix_x.toarray() or numpy array\n",
    "# labels are the list of attributes that are assigned by classifier\n",
    "# lower value : objects/docs are close to its centroid, are being clustered well\n",
    "\n",
    "print(davies_bouldin_score(matrix_x.toarray(), km.labels_))\n",
    "\n",
    "# range [-1, 1] \n",
    "print(silhouette_score(matrix_x.toarray(), km.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('sentiDataset.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = corpus.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get Rating and Reviews in a separate list\n",
    "ratings = []\n",
    "reviews = []\n",
    "\n",
    "for document in docs_list:\n",
    "\n",
    "    d = document.split('\\t')\n",
    "    ratings.append(d[2])    \n",
    "    reviews.append(d[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=200, ngram_range=(1,3)).fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn1 = KNeighborsClassifier(n_neighbors = 1, weights = 'distance')\n",
    "knn2 = KNeighborsClassifier(n_neighbors = 2, algorithm = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1 = DecisionTreeClassifier(max_depth = 2)\n",
    "dt2 = DecisionTreeClassifier(max_depth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ratings = np.array(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/momina/miniconda3/envs/stats/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/momina/miniconda3/envs/stats/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/momina/miniconda3/envs/stats/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/momina/miniconda3/envs/stats/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/momina/miniconda3/envs/stats/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "p_score1, p_score2, p_score3, p_score4, p_score5 = [],[],[],[],[]\n",
    "r_score1, r_score2, r_score3, r_score4, r_score5 = [],[],[],[],[]\n",
    "f1_score1, f1_score2, f1_score3, f1_score4, f1_score5 = [],[],[],[],[]\n",
    "\n",
    "for train_ids, test_ids in kf.split(tfidf, ratings):\n",
    "    train_reviews, test_reviews = tfidf[train_ids], tfidf[test_ids]\n",
    "    train_ratings, test_ratings = ratings[train_ids], ratings[test_ids]\n",
    "    \n",
    "    k1 = knn1.fit(train_reviews, train_ratings)\n",
    "    k2 = knn2.fit(train_reviews, train_ratings)\n",
    "    \n",
    "    d1 = dt1.fit(train_reviews, train_ratings)\n",
    "    d2 = dt2.fit(train_reviews, train_ratings)\n",
    "    \n",
    "    n = nb.fit(train_reviews, train_ratings)\n",
    "    \n",
    "    predicted_ratings1 = k1.predict(test_reviews)\n",
    "    predicted_ratings2 = k2.predict(test_reviews)\n",
    "    predicted_ratings3 = d1.predict(test_reviews)\n",
    "    predicted_ratings4 = d2.predict(test_reviews)\n",
    "    predicted_ratings5 = n.predict(test_reviews)\n",
    "    \n",
    "    p_score1.append(precision_score(test_ratings, predicted_ratings1, average='weighted'))\n",
    "    r_score1.append(recall_score(test_ratings, predicted_ratings1, average='weighted'))\n",
    "    f1_score1.append(f1_score(test_ratings, predicted_ratings1, average='micro'))\n",
    "    cm1 = confusion_matrix(test_ratings, predicted_ratings1)\n",
    "    \n",
    "    p_score2.append(precision_score(test_ratings, predicted_ratings2, average='weighted'))\n",
    "    r_score2.append(recall_score(test_ratings, predicted_ratings2, average='weighted'))\n",
    "    f1_score2.append(f1_score(test_ratings, predicted_ratings2, average='micro'))\n",
    "    cm2 = confusion_matrix(test_ratings, predicted_ratings2)\n",
    "    \n",
    "    p_score3.append(precision_score(test_ratings, predicted_ratings3, average='weighted'))\n",
    "    r_score3.append(recall_score(test_ratings, predicted_ratings3, average='weighted'))\n",
    "    f1_score3.append(f1_score(test_ratings, predicted_ratings3, average='micro'))\n",
    "    cm3 = confusion_matrix(test_ratings, predicted_ratings3)\n",
    "    \n",
    "    p_score4.append(precision_score(test_ratings, predicted_ratings4, average='weighted'))\n",
    "    r_score4.append(recall_score(test_ratings, predicted_ratings4, average='weighted'))\n",
    "    f1_score4.append(f1_score(test_ratings, predicted_ratings4, average='micro'))\n",
    "    cm4 = confusion_matrix(test_ratings, predicted_ratings4)\n",
    "    \n",
    "    p_score5.append(precision_score(test_ratings, predicted_ratings5, average='weighted'))\n",
    "    r_score5.append(recall_score(test_ratings, predicted_ratings5, average='weighted'))\n",
    "    f1_score5.append(f1_score(test_ratings, predicted_ratings5, average='micro'))\n",
    "    cm5 = confusion_matrix(test_ratings, predicted_ratings5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for knn1 ---  Precision: 0.41496297851160635 Recall: 0.40515 F1: 0.40515\n",
      "Scores for knn2 ---  Precision: 0.43362257399079773 Recall: 0.32415 F1: 0.32415\n",
      "Scores for dt1  ---  Precision: 0.3059755125        Recall: 0.55275 F1: 0.55275\n",
      "Scores for dt2  ---  Precision: 0.3138672879089306  Recall: 0.5516  F1: 0.5516\n",
      "Scores for nb   ---  Precision: 0.3344454708777209  Recall: 0.5529  F1: 0.5529\n"
     ]
    }
   ],
   "source": [
    "print('Scores for knn1 --- ','Precision:',sum(p_score1)/len(p_score1), 'Recall:',sum(r_score1)/len(r_score1),'F1:',sum(f1_score1)/len(f1_score1))\n",
    "print('Scores for knn2 --- ','Precision:',sum(p_score2)/len(p_score2), 'Recall:',sum(r_score2)/len(r_score2),'F1:',sum(f1_score2)/len(f1_score2))\n",
    "print('Scores for dt1  --- ','Precision:',sum(p_score3)/len(p_score3), '       Recall:',sum(r_score3)/len(r_score3),'F1:',sum(f1_score3)/len(f1_score3))\n",
    "print('Scores for dt2  --- ','Precision:',sum(p_score4)/len(p_score4), ' Recall:',sum(r_score4)/len(r_score4),' F1:',sum(f1_score4)/len(f1_score4))\n",
    "print('Scores for nb   --- ','Precision:',sum(p_score5)/len(p_score5), ' Recall:',sum(r_score5)/len(r_score5),' F1:',sum(f1_score5)/len(f1_score5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb model is doing better than other models as it has the highest F1 and Recall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for knn1:\n",
      " [[  66   27   41  116  150]\n",
      " [  34    7   40   84   90]\n",
      " [  26   19   41   89  130]\n",
      " [  75   31   72  220  335]\n",
      " [ 167   70  172  605 1293]] \n",
      "\n",
      "Confusion matrix for knn2:\n",
      " [[105  36  59 114  86]\n",
      " [ 53  20  55  84  43]\n",
      " [ 56  29  62  94  64]\n",
      " [129  56 110 236 202]\n",
      " [305 122 285 758 837]] \n",
      "\n",
      "Confusion matrix for dt1:\n",
      " [[   0    0    0    0  400]\n",
      " [   0    0    0    0  255]\n",
      " [   0    0    0    0  305]\n",
      " [   0    0    0    0  733]\n",
      " [   0    0    0    0 2307]] \n",
      "\n",
      "Confusion matrix for dt2:\n",
      " [[   0    0    0    0  400]\n",
      " [   0    0    0    0  255]\n",
      " [   0    0    0    0  305]\n",
      " [   0    0    0    0  733]\n",
      " [   0    0    0    0 2307]] \n",
      "\n",
      "Confusion matrix for nb:\n",
      " [[   6    0    0    0  394]\n",
      " [   3    0    0    0  252]\n",
      " [   0    0    0    0  305]\n",
      " [   1    0    0    0  732]\n",
      " [   5    0    0    0 2302]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Confusion matrix for knn1:\\n',cm1,'\\n')\n",
    "print('Confusion matrix for knn2:\\n',cm2,'\\n')\n",
    "print('Confusion matrix for dt1:\\n',cm3,'\\n')\n",
    "print('Confusion matrix for dt2:\\n',cm4,'\\n')\n",
    "print('Confusion matrix for nb:\\n',cm5,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
