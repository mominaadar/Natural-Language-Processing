{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 Implementation of LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['bread bread bread bread bread bread bread bread bread bread',\n",
    "          'milk milk milk milk milk milk milk milk milk milk',\n",
    "          'pet pet pet pet pet pet pet pet pet pet',\n",
    "          'bread bread bread bread bread bread bread bread bread bread milk milk milk milk milk milk milk milk milk milk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x3 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_x = vec.fit_transform(corpus)\n",
    "matrix_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  0,  0],\n",
       "       [ 0, 10,  0],\n",
       "       [ 0,  0, 10],\n",
       "       [10, 10,  0]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_components : number of topics\n",
    "lda = LatentDirichletAllocation(n_components = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=2)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(matrix_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.45052273, 20.49672838,  0.50135735],\n",
       "       [10.54947727,  0.50327162, 10.49864265]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# components that LDA has extracted basically the topics\n",
    "# array[0] : 1st topic, array[1] : 2nd topic\n",
    "# values represent relevance of each word with each topic\n",
    "# soft clustering as all the words belong to all the topics (because values are not 0)\n",
    "# values are probs, not normalized\n",
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bread': 0, 'milk': 1, 'pet': 2}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "word IDs: [1 0 2]\n",
      "words: ['milk', 'bread', 'pet']\n",
      "prob: [20.49672838354807, 10.450522734138774, 0.5013573459416129]\n",
      "----------\n",
      "topic: 1\n",
      "word IDs: [0 2 1]\n",
      "words: ['bread', 'pet', 'milk']\n",
      "prob: [10.549477265861215, 10.498642654058381, 0.5032716164519231]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# these words in a topic have strong corelation with eachother\n",
    "\n",
    "for topic_id, topic in enumerate(lda.components_):\n",
    "    print('topic:',topic_id)\n",
    "    \n",
    "    # to see word IDs in each topic\n",
    "#     print('word IDs:',topic.argsort()[::-1])\n",
    "#     print('words:',[features[i] for i in topic.argsort()])\n",
    "#     print('prob:',[topic[i] for i in topic.argsort()])\n",
    "\n",
    "    # to get probs in descending order\n",
    "    print('word IDs:',topic.argsort()[::-1])\n",
    "    print('words:',[features[i] for i in topic.argsort()[::-1]])\n",
    "    print('prob:',[topic[i] for i in topic.argsort()[::-1]])\n",
    "    print('----------')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 Practical w/ topic modeling  on UCI repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('dataset2.csv', encoding='Latin-1', errors='ignore').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = corpus.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_x = vec.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_components = 6 : to soft cluster 2457 features^ in 6 clusters\n",
    "lda = LatentDirichletAllocation(n_components = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=6)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(matrix_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16667054, 0.16667115, 1.43506932, ..., 0.16666861, 0.16667798,\n",
       "        0.16666862],\n",
       "       [0.38845029, 0.16667161, 0.16667374, ..., 0.16666878, 0.90893044,\n",
       "        0.16666883],\n",
       "       [0.16666894, 0.73995469, 0.16998067, ..., 0.6662572 , 0.17834019,\n",
       "        0.16666782],\n",
       "       [0.16667293, 0.25304891, 0.16667721, ..., 0.16666984, 0.16668512,\n",
       "        0.16666989],\n",
       "       [0.16667193, 0.81513356, 0.16754553, ..., 0.16716248, 0.418654  ,\n",
       "        0.16666943],\n",
       "       [0.48677208, 0.16667243, 0.45149646, ..., 0.16666909, 0.16668098,\n",
       "        0.59446138]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "word IDs: [2450 2366 2212 2176  183  418 2402  108 2180]\n",
      "word: ['you', 'we', 'to', 'thank', 'back', 'come', 'will', 'and', 'the']\n",
      "prob: [14.054962674997867, 11.03380166915811, 9.840805811325547, 9.216784656611988, 9.015972235779595, 8.623706022654037, 7.900634288573023, 7.363489555331691, 6.353311429442956]\n",
      "-----------\n",
      "topic: 1\n",
      "word IDs: [2180  108 2327 1471 2422 2414 2450 1176 2212]\n",
      "word: ['the', 'and', 'very', 'nice', 'wonderful', 'with', 'you', 'is', 'to']\n",
      "prob: [7.133862391863483, 6.30406582807773, 6.201338192644753, 4.831321568984839, 4.649677338075756, 4.560977246144773, 4.366533946019295, 4.238494131511153, 4.212166337229894]\n",
      "-----------\n",
      "topic: 2\n",
      "word IDs: [2180  108 2366 2212  894 1176 1512 2450 2355]\n",
      "word: ['the', 'and', 'we', 'to', 'for', 'is', 'of', 'you', 'was']\n",
      "prob: [45.62949719712179, 26.541864876530042, 22.791322876233355, 20.56242191726232, 17.9615040901952, 16.431664798169834, 15.113422665286507, 14.914866652927355, 14.530533336895674]\n",
      "-----------\n",
      "topic: 3\n",
      "word IDs: [2324 1333   46  793  506 1849  607 1898 1702]\n",
      "word: ['vegetarian', 'main', 'across', 'expensive', 'course', 'review', 'diner', 'sea', 'price']\n",
      "prob: [1.607094977571547, 1.4105955725381674, 1.2341738470764239, 1.2165891365493275, 1.1894596938827235, 1.1671954191307157, 1.1272678366358142, 1.0545852052544638, 1.0453839168808141]\n",
      "-----------\n",
      "topic: 4\n",
      "word IDs: [2180  108 1126 1569 1442  374 1281 1176 2291]\n",
      "word: ['the', 'and', 'in', 'over', 'my', 'children', 'll', 'is', 'unique']\n",
      "prob: [3.1146282691465204, 1.7971722599074151, 1.5364511737521453, 1.4657391302533525, 1.4005495559456127, 1.358249116638425, 1.348566684635388, 1.2970992710548024, 1.2856903724030473]\n",
      "-----------\n",
      "topic: 5\n",
      "word IDs: [ 108 2180 2422 2212  906  210  548 2384 1512]\n",
      "word: ['and', 'the', 'wonderful', 'to', 'forward', 'beautiful', 'dear', 'what', 'of']\n",
      "prob: [3.4895668493546386, 3.1746417312697552, 2.680765651630072, 2.6660508991457186, 2.4947698623478844, 2.481112489782727, 2.313319810510472, 2.2676920752881298, 2.2497659968259676]\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic in enumerate(lda.components_):\n",
    "    print('topic:',topic_id)\n",
    "    print('word IDs:',topic.argsort()[:-10:-1])\n",
    "    print('word:',[features[i] for i in topic.argsort()[:-10:-1]])\n",
    "    print('prob:',[topic[i] for i in topic.argsort()[:-10:-1]])\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 Implementing LDA w/ different hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['bread bread bread bread bread bread bread bread bread bread',\n",
    "          'milk milk milk milk milk milk milk milk milk milk',\n",
    "          'pet pet pet pet pet pet pet pet pet pet',\n",
    "          'bread bread bread bread bread bread bread bread bread bread milk milk milk milk milk milk milk milk milk milk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "matrix_x = vec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(doc_topic_prior=0.01, n_components=2,\n",
       "                          topic_word_prior=1.0)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# doc_topic_prior : alpha, topic_word_prior : beta\n",
    "# if we decrease topic_word_prior value then the difference b/w the highest prob_val and lowest prob_val in a topic increases\n",
    "# if we increase the topic_prior_value then the difference b/w highest prob_val and lowest prob_val in a topic decreases\n",
    "\n",
    "# if we increase doc_topic_prior then the different b/w values in transform will decrease i.e. supporting more topics with similar probs to represent a topic\n",
    "# if we decrease doc_topic_prior then the difference b/w values in transform will increase i.e. supporting fewer topics within the document, docs represented by few no. of topics with higher probs\n",
    "lda = LatentDirichletAllocation(n_components = 2, topic_word_prior = 1.0, doc_topic_prior=0.01)\n",
    "lda.fit(matrix_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.4532381 , 16.36454154,  3.56617907],\n",
       "       [13.5467619 ,  5.63545846,  8.43382093]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.36454154468869, 8.453238103397524, 3.5661790723599482]\n",
      "[13.546761896602463, 8.433820927640042, 5.635458455311296]\n"
     ]
    }
   ],
   "source": [
    "for topic in lda.components_:\n",
    "    print([topic[t] for t in topic.argsort()[::-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99001996e-01, 9.98003992e-04],\n",
       "       [9.98003992e-04, 9.99001996e-01],\n",
       "       [9.99001996e-01, 9.98003992e-04],\n",
       "       [4.99500500e-04, 9.99500500e-01]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see documents topic distribution\n",
    "# higher value in a doc represents the topic that has more influence in it\n",
    "lda.transform(matrix_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.4 Online LDA with UCI Repo Dtaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('dataset2.csv', encoding='Latin-1', errors='ignore').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = corpus.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "matrix_x = vec.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# we want our model to learn incrementally when data is too big so 'online'\n",
    "# learning_offset: to keep in check how much we want our model to modify after receiving another batch, val should be +ve\n",
    "lda = LatentDirichletAllocation(n_components = 2, max_iter = 200, learning_method='online', learning_offset=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', max_iter=200,\n",
       "                          n_components=2)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partial_fit : the model will retain info what it learn in each iteration and doesn't retrain model in each iteration\n",
    "lda.partial_fit(matrix_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.18206652,  6.32160862,  1.16951975, ...,  0.50322718,\n",
       "         0.74336439,  1.33158447],\n",
       "       [ 0.52428204,  0.56787265,  3.21961333, ...,  3.06230048,\n",
       "        10.45770662,  0.52158981]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want to provide batch of data to partial_fit so dividing the data\n",
    "step = matrix_x.shape[0]/10\n",
    "step = int(step)\n",
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration: 0\n",
      "[[0.93031017 0.95506432 0.74711555 ... 0.69658036 0.82972445 0.78840153]\n",
      " [0.79832566 0.87657044 0.85943474 ... 0.8314769  0.83904556 0.77254225]]\n",
      "\n",
      "iteration: 1\n",
      "[[8.07544843e-01 8.25236758e-01 4.18241698e+03 ... 6.40496972e-01\n",
      "  7.35655719e-01 7.06122021e-01]\n",
      " [7.13214845e-01 7.69136786e-01 2.08169996e+02 ... 7.36908203e-01\n",
      "  7.42317566e-01 6.94787318e-01]]\n",
      "\n",
      "iteration: 2\n",
      "[[7.28778656e-01 7.41939444e-01 3.11137430e+03 ... 6.04513892e-01\n",
      "  6.75301261e-01 6.53331523e-01]\n",
      " [6.58607783e-01 7.00207396e-01 1.54983041e+02 ... 6.76232967e-01\n",
      "  6.80256923e-01 6.44899782e-01]]\n",
      "\n",
      "iteration: 3\n",
      "[[6.75414148e-01 2.13733565e+04 2.38573721e+03 ... 1.04548280e+03\n",
      "  4.67939885e+03 6.17565680e-01]\n",
      " [6.21611209e-01 3.74810135e+03 1.18948597e+02 ... 2.54432086e+03\n",
      "  2.49905058e+03 6.11100713e-01]]\n",
      "\n",
      "iteration: 4\n",
      "[[6.37735539e-01 1.67825096e+04 5.11642849e+03 ... 8.21022582e+02\n",
      "  3.67438071e+03 5.92312807e-01]\n",
      " [5.95489364e-01 2.94312406e+03 1.55055287e+02 ... 1.99791321e+03\n",
      "  1.96236690e+03 5.87236501e-01]]\n",
      "\n",
      "iteration: 5\n",
      "[[6.10253686e-01 1.34340585e+04 7.16362667e+03 ... 6.57306803e+02\n",
      "  2.94134514e+03 5.73893980e-01]\n",
      " [5.76436731e-01 2.35599337e+03 1.25891673e+02 ... 1.59937688e+03\n",
      "  1.57092299e+03 5.69830531e-01]]\n",
      "\n",
      "iteration: 6\n",
      "[[5.89674971e-01 1.09267014e+04 5.82663793e+03 ... 5.34714625e+02\n",
      "  2.39244003e+03 5.60101760e-01]\n",
      " [5.62169910e-01 1.91634344e+03 1.02487472e+02 ... 1.30094849e+03\n",
      "  1.27780548e+03 5.56796749e-01]]\n",
      "\n",
      "iteration: 7\n",
      "[[5100.53663136 9007.84561309 4803.45357431 ...  440.896035\n",
      "  1972.36833738 2600.44916856]\n",
      " [ 304.27027328 1579.88367035   84.57646712 ... 1072.56416768\n",
      "  1053.48552641  102.48805927]]\n",
      "\n",
      "iteration: 8\n",
      "[[4253.67404051 7512.17321034 6466.23236391 ... 1261.74800469\n",
      "  1644.93901683 2168.72684025]\n",
      " [ 253.82912957 1317.62654482  164.92482815 ... 5109.8076025\n",
      "   878.63696835   85.55291187]]\n",
      "\n",
      "iteration: 9\n",
      "[[3044.94191653 5377.39089771 4628.7015366  ...  903.30723432\n",
      "  1435.05995146 1552.52693662]\n",
      " [ 181.83417852  943.3053553   118.19606279 ... 3657.76633364\n",
      "  4652.91582862   61.38127303]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "for i in range(10):\n",
    "    if i == 9:\n",
    "        lda.partial_fit(matrix_x[index:])\n",
    "    lda.partial_fit(matrix_x[index:index+step])\n",
    "    index = index + step\n",
    "    \n",
    "    print('\\niteration:',i)\n",
    "    print(lda.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.5 Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('dataset2.csv', encoding='Latin-1', errors='ignore').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = corpus.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(653, 2457)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "matrix_x = vec.fit_transform(docs)\n",
    "matrix_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda1 = LatentDirichletAllocation(n_components = 2)\n",
    "lda2 = LatentDirichletAllocation(n_components = 3)\n",
    "lda3 = LatentDirichletAllocation(n_components = 4)\n",
    "lda4 = LatentDirichletAllocation(n_components = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=5)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda1.fit(matrix_x[:500])\n",
    "lda2.fit(matrix_x[:500])\n",
    "lda3.fit(matrix_x[:500])\n",
    "lda4.fit(matrix_x[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1215.75640830575\n",
      "1648.015356114946\n",
      "2587.477503265197\n",
      "2969.2546860247135\n"
     ]
    }
   ],
   "source": [
    "# components = 2 gave us more compact topics as compared to components = 3 cuz val less\n",
    "# we dont know what n_components is good value so we train models on different n_components and select the one which has the least val of perpelexity i.e. closer to 0\n",
    "print(lda1.perplexity(matrix_x[500:]))\n",
    "print(lda2.perplexity(matrix_x[500:]))\n",
    "print(lda3.perplexity(matrix_x[500:]))\n",
    "print(lda4.perplexity(matrix_x[500:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('dataset2.csv', encoding='Latin-1', errors='ignore').read()\n",
    "docs = corpus.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "matrix_x = tfidf.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=20)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components = 20)\n",
    "lda.fit(matrix_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic ID: 0\n",
      "Words: ['love', 'as', 'forward', 'great', 'pass']\n",
      "Probabilities: [3.7069996676180694, 1.2542498252912746, 0.8499686414956816, 0.6498166177385032, 0.6146883462193375]\n",
      "--------\n",
      "Topic ID: 1\n",
      "Words: ['as', 'company', 'none', 'tasting', 'evening']\n",
      "Probabilities: [0.8809846092557523, 0.6785104994300863, 0.5843270889203412, 0.5730756805287434, 0.5723567865163756]\n",
      "--------\n",
      "Topic ID: 2\n",
      "Words: ['nearby', 'said', 'carry', 'beach', 'run']\n",
      "Probabilities: [0.8165578198914392, 0.6921319465491572, 0.6139116082599693, 0.5685004475804897, 0.5676947056428434]\n",
      "--------\n",
      "Topic ID: 3\n",
      "Words: ['recommend', 'second', 'heaven', 'still', 'cool']\n",
      "Probabilities: [1.1994615665817536, 1.0242910913875023, 0.9065667159460022, 0.8720400544027536, 0.8652566497105554]\n",
      "--------\n",
      "Topic ID: 4\n",
      "Words: ['having', '1000', 'sublime', 'leonardo', 'ola']\n",
      "Probabilities: [1.7735851965956897, 1.0949201660151646, 0.8892188579701752, 0.8769228422457807, 0.8750732075214527]\n",
      "--------\n",
      "Topic ID: 5\n",
      "Words: ['feedback', 'if', 'far', 'more', 'some']\n",
      "Probabilities: [1.139313379391368, 0.9076577277344651, 0.6380970045873483, 0.6301921565389739, 0.6029857139817285]\n",
      "--------\n",
      "Topic ID: 6\n",
      "Words: ['favorite', 'welcome', 'delighted', 'pro', 'as']\n",
      "Probabilities: [1.2581507595783732, 1.2229633274495397, 1.1861543080911734, 1.0499999999998328, 0.8083208167882814]\n",
      "--------\n",
      "Topic ID: 7\n",
      "Words: ['super', 'love', 'available', 'pretty', 'life']\n",
      "Probabilities: [1.113821130848759, 1.0672798434754216, 0.78679235171545, 0.7003682628736507, 0.6789714442712566]\n",
      "--------\n",
      "Topic ID: 8\n",
      "Words: ['stayed', 'kitchen', 'open', 'menu', 'yet']\n",
      "Probabilities: [1.8866187981817686, 1.3132187437103182, 1.2832470158992304, 1.2732277532814513, 1.0984757935493676]\n",
      "--------\n",
      "Topic ID: 9\n",
      "Words: ['pleasant', 'repeat', 'hello', 'any', 'real']\n",
      "Probabilities: [2.3960416131671005, 2.281045097413007, 1.73502008085738, 1.331171275758642, 1.3248507770155793]\n",
      "--------\n",
      "Topic ID: 10\n",
      "Words: ['fabulous', 'expensive', 'dear', 'diner', 'relax']\n",
      "Probabilities: [1.446891521421803, 1.1219363326679435, 1.10346610833678, 1.0094913697165975, 0.9779235636308714]\n",
      "--------\n",
      "Topic ID: 11\n",
      "Words: ['great', 'delicious', 'breakfast', 'as', 'would']\n",
      "Probabilities: [4.747588084147038, 4.407254894199927, 3.6691599939855988, 3.437942291369458, 3.267859793621441]\n",
      "--------\n",
      "Topic ID: 12\n",
      "Words: ['joana', 'look', 'forward', 'cutting', 'her']\n",
      "Probabilities: [1.1472005866211854, 0.8433831619488419, 0.8299741712451891, 0.6030360667915297, 0.5744597371619283]\n",
      "--------\n",
      "Topic ID: 13\n",
      "Words: ['great', 'guests', 'passionate', 'truly', 'from']\n",
      "Probabilities: [0.8016962579805648, 0.7738556033275004, 0.690151655010578, 0.6412848538805244, 0.5964814873282052]\n",
      "--------\n",
      "Topic ID: 14\n",
      "Words: ['gold', 'main', 'course', 'than', 'away']\n",
      "Probabilities: [1.4652002418550092, 1.3202608136525926, 1.1326449490806458, 1.0299231973358918, 1.0237744184761646]\n",
      "--------\n",
      "Topic ID: 15\n",
      "Words: ['regards', 'wonder', 'menu', 'kind', 'but']\n",
      "Probabilities: [1.24933977563898, 1.0500000634765267, 0.9551293109527872, 0.8127130175868492, 0.7217792073484789]\n",
      "--------\n",
      "Topic ID: 16\n",
      "Words: ['week', 'just', 'magnify', 'congrats', 'senses']\n",
      "Probabilities: [1.6476976400966925, 0.983603323612962, 0.8832557184190907, 0.849504284516759, 0.7606795243552207]\n",
      "--------\n",
      "Topic ID: 17\n",
      "Words: ['the', 'and', 'we', 'to', 'you']\n",
      "Probabilities: [62.42769200766663, 41.807002979878014, 38.16836540948051, 37.6952345751344, 34.491923348043706]\n",
      "--------\n",
      "Topic ID: 18\n",
      "Words: ['nature', 'suggestion', 'guys', 'shall', 'keep']\n",
      "Probabilities: [0.7172068595395628, 0.6864926725199914, 0.6715682698223927, 0.6012349484128943, 0.5840115698335431]\n",
      "--------\n",
      "Topic ID: 19\n",
      "Words: ['no', 'doubt', 'tasteful', 'repeat', 'liked']\n",
      "Probabilities: [1.2908009944303718, 1.2854859303749997, 1.1092847274552724, 1.0381949161740442, 1.0005714458940451]\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "features = tfidf.get_feature_names()\n",
    "\n",
    "for topic_id, topic in enumerate(lda.components_):\n",
    "    print('Topic ID:',topic_id)\n",
    "    print('Words:', [features[i] for i in topic.argsort()[:-6:-1]])\n",
    "    print('Probabilities:', [topic[i] for i in topic.argsort()[:-6:-1]])\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
