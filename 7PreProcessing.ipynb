{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 - 6.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '  This is my duMmy Dataset. It is part of taught course on text mining.   '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  this is my dummy dataset. it is part of taught course on text mining.   '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. convert text in one case; lowerCase is preffered\n",
    "\n",
    "corpus = corpus.lower()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is my dummy dataset. it is part of taught course on text mining.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. remove white spaces\n",
    "# strip() removes something from before and after the text\n",
    "\n",
    "corpus = corpus.strip()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is my dummy dataset it is part of taught course on text mining'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. remove puncuations\n",
    "\n",
    "from string import punctuation as punc\n",
    "\n",
    "for ch in punc:\n",
    "    if ch in corpus:\n",
    "        corpus = corpus.replace(ch,'')\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'dummy', 'dataset', 'is', 'of', 'taught', 'course', 'text', 'mining']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. remove stop words\n",
    "\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "# tokenize corpus to check with stop_words list\n",
    "words = corpus.split(' ')\n",
    "\n",
    "for word in words:\n",
    "    if word in ENGLISH_STOP_WORDS:\n",
    "        words.remove(word)\n",
    "        \n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 276 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/momina/miniconda3/envs/stats/lib/python3.8/site-packages (from nltk) (0.16.0)\n",
      "Collecting regex\n",
      "  Downloading regex-2021.4.4-cp38-cp38-manylinux2014_x86_64.whl (733 kB)\n",
      "\u001b[K     |████████████████████████████████| 733 kB 507 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-7.1.2 nltk-3.6.2 regex-2021.4.4 tqdm-4.60.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Stemming\n",
    "# goal is to convert same words with different styles to the basic form\n",
    "# e.g. stable, stabilize, stability, stabilizes = convert them to stable\n",
    "# stemming chops off \n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "pstemmer = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'dummi', 'dataset', 'is', 'of', 'taught', 'cours', 'text', 'mine']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0,len(words)):\n",
    "    words[i] = pstemmer.stem(words[i])\n",
    "    \n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/momina/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be', 'dummi', 'dataset', 'be', 'of', 'teach', 'cours', 'text', 'mine']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is, are, am --> be\n",
    "# taught --> teach\n",
    "# lemmatization: covert words to their 1st form\n",
    "\n",
    "for i in range(0, len(words)):\n",
    "    words[i] = lemma.lemmatize(words[i], pos='v')\n",
    "    \n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4 Applying RegEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '<html><head></head><body><h1>Paragraph Heading</h1><pThis is some text. <a href=\"\">The original price was $500 but now only USD250 </a> This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is <em>some text.</em> <strong>This is some text.</strong> This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. </p></body></html>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html><head></head><body><h1>Paragraph Heading</h1><pThis is some text. <a href=\"\">The original price was $500 but now only USD250 </a> This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is <em>some text.</em> <strong>This is some text.</strong> This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. </p></body></html>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove tags\n",
    "tags = re.compile(r'<.*?>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paragraph HeadingThe original price was $500 but now only USD250  This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sub : substitution\n",
    "# ('', corpus) : '' means substitute with ''/nothing and apply on corpus\n",
    "\n",
    "corpus = tags.sub('', corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# + : maybe one or more\n",
    "prices = re.compile(r'(USD|\\$)[0-9]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paragraph HeadingThe original price was  but now only   This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = prices.sub('',corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.5 Parts-of-speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'i like this table in my room'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/momina/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'like', 'this', 'table', 'in', 'my', 'room']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = corpus.split(' ')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('table', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('room', 'NN')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words - POS\n",
    "tags = pos_tag(words)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'DT')\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(tags)):\n",
    "    if tags[i][1] == 'DT':\n",
    "        print(tags[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i like this table in my room. it is made of bamboo wood. it is dark brown in color and has a polished finish. i have bought a chair with it to sit and study.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = 'i like this table in my room. it is made of bamboo wood. it is dark brown in color and has a polished finish. i have bought a chair with it to sit and study.'\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like this table in my room',\n",
       " 'it is made of bamboo wood',\n",
       " 'it is dark brown in color and has a polished finish',\n",
       " 'i have bought a chair with it to sit and study']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = corpus.split('.')\n",
    "\n",
    "sentences=[]\n",
    "for i in sen:\n",
    "    if i != '':\n",
    "        i = i.strip()\n",
    "        sentences.append(i)\n",
    "        \n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns: ('i', 'NN')\n",
      "Nouns: ('table', 'NN')\n",
      "Nouns: ('room', 'NN')\n",
      "Nouns: ('bamboo', 'NN')\n",
      "Nouns: ('wood', 'NN')\n",
      "Nouns: ('brown', 'NN')\n",
      "Nouns: ('color', 'NN')\n",
      "Nouns: ('finish', 'NN')\n",
      "Nouns: ('chair', 'NN')\n"
     ]
    }
   ],
   "source": [
    "for sen in sentences:\n",
    "    doc = sen.split(' ')\n",
    "    \n",
    "    tags = pos_tag(doc)\n",
    "    \n",
    "    for i in range(0, len(tags)):\n",
    "        if tags[i][1] == 'NN':\n",
    "            print('Nouns:',tags[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.6 Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to forget previous connections, to lose proxy related info\n",
    "\n",
    "session.trust_env = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for getting data of webpage\n",
    "\n",
    "response = session.get('http://cstags.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you're behind proxies then do this\n",
    "\n",
    "proxies = {'http':'http://192.168.1.23:179',\n",
    "           'https':''}\n",
    "\n",
    "response = session.get('http://cstags.pk', proxies = {'http':'http://192.168.1.23:179','https':''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.7 Text Segmentation and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'He owes me 22.50 dollars. Which is due by the next day.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He owes me 22', '50 dollars', ' Which is due by the next day', '']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem with splitting on the basis of . using split()\n",
    "sen = corpus.split('.')\n",
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/momina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He owes me 22.50 dollars.', 'Which is due by the next day.']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'owes',\n",
       " 'me',\n",
       " '22.50',\n",
       " 'dollars',\n",
       " '.',\n",
       " 'Which',\n",
       " 'is',\n",
       " 'due',\n",
       " 'by',\n",
       " 'the',\n",
       " 'next',\n",
       " 'day',\n",
       " '.']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'owes', 'me', '22.50', 'dollars', '.']\n",
      "['Which', 'is', 'due', 'by', 'the', 'next', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(corpus):\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from string import punctuation as punc\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('buttons_amazon_kindle.txt.data', encoding='Latin-1').read()\n",
    "corpus = corpus.lower()\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "\n",
    "for char in punc:\n",
    "    if char in corpus:\n",
    "        corpus = corpus.replace(char,'')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "\n",
    "words = corpus.split(' ')\n",
    "\n",
    "for w in words:\n",
    "    if w == '' or w == '\\n':\n",
    "        words.remove(w)\n",
    "        \n",
    "    if w in ENGLISH_STOP_WORDS:\n",
    "        words.remove(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS is not noun or verb\n",
    "\n",
    "tags = pos_tag(words)\n",
    "# tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tags = []\n",
    "\n",
    "for i in range(0, len(tags)):\n",
    "    if tags[i][1] != 'NN' and (tags[i][1][0] != 'V' and tags[i][1][1] != 'B' ):\n",
    "        n_tags.append(tags[i])\n",
    "        \n",
    "# n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
