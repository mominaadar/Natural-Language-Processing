{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 - 6.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '  This is my duMmy Dataset. It is part of taught course on text mining.   '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  this is my dummy dataset. it is part of taught course on text mining.   '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. convert text in one case; lowerCase is preffered\n",
    "\n",
    "corpus = corpus.lower()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is my dummy dataset. it is part of taught course on text mining.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. remove white spaces\n",
    "# strip() removes something from before and after the text\n",
    "\n",
    "corpus = corpus.strip()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is my dummy dataset it is part of taught course on text mining'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. remove puncuations\n",
    "\n",
    "from string import punctuation as punc\n",
    "\n",
    "for ch in punc:\n",
    "    if ch in corpus:\n",
    "        corpus = corpus.replace(ch,'')\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'dummy', 'dataset', 'is', 'of', 'taught', 'course', 'text', 'mining']\n"
     ]
    }
   ],
   "source": [
    "# 4. remove stop words\n",
    "\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "# tokenize corpus to check with stop_words list\n",
    "words = corpus.split(' ')\n",
    "# print(words)\n",
    "\n",
    "for word in words:\n",
    "    if word in ENGLISH_STOP_WORDS:\n",
    "        words.remove(word)\n",
    "        \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Stemming\n",
    "# goal is to convert same words with different styles to the basic form\n",
    "# e.g. stable, stabilize, stability, stabilizes = convert them to stable\n",
    "# stemming chops off \n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "pstemmer = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'dummi', 'dataset', 'is', 'of', 'taught', 'cours', 'text', 'mine']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0,len(words)):\n",
    "    words[i] = pstemmer.stem(words[i])\n",
    "    \n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be', 'dummi', 'dataset', 'be', 'of', 'teach', 'cours', 'text', 'mine']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is, are, am --> be\n",
    "# taught --> teach\n",
    "# lemmatization: covert words to their 1st form\n",
    "\n",
    "for i in range(0, len(words)):\n",
    "    words[i] = lemma.lemmatize(words[i], pos='v')\n",
    "    \n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4 Applying RegEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '<html><head></head><body><h1>Paragraph Heading</h1><pThis is some text. <a href=\"\">The original price was $500 but now only USD250 </a> This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is <em>some text.</em> <strong>This is some text.</strong> This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. </p></body></html>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html><head></head><body><h1>Paragraph Heading</h1><pThis is some text. <a href=\"\">The original price was $500 but now only USD250 </a> This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is <em>some text.</em> <strong>This is some text.</strong> This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. </p></body></html>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove tags\n",
    "tags = re.compile(r'<.*?>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paragraph HeadingThe original price was $500 but now only USD250  This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sub : substitution\n",
    "# ('', corpus) : '' means substitute with ''/nothing and apply on corpus\n",
    "\n",
    "corpus = tags.sub('', corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# + : maybe one or more\n",
    "# starting with either USD or $ and followed by at least one digit or more than one digit\n",
    "prices = re.compile(r'(USD|\\$)[0-9]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paragraph HeadingThe original price was  but now only   This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = prices.sub('',corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.5 Parts-of-speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'i like this table in my room'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'like', 'this', 'table', 'in', 'my', 'room']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = corpus.split(' ')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('table', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('room', 'NN')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CC\tcoordinating conjunction\n",
    "# CD\tcardinal digit\n",
    "# DT\tdeterminer\n",
    "# EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "# FW\tforeign word\n",
    "# IN\tpreposition/subordinating conjunction\n",
    "# JJ\tadjective\t'big'\n",
    "# JJR\tadjective, comparative\t'bigger'\n",
    "# JJS\tadjective, superlative\t'biggest'\n",
    "# LS\tlist marker\t1)\n",
    "# MD\tmodal\tcould, will\n",
    "# NN\tnoun, singular 'desk'\n",
    "# NNS\tnoun plural\t'desks'\n",
    "# NNP\tproper noun, singular\t'Harrison'\n",
    "# NNPS\tproper noun, plural\t'Americans'\n",
    "# PDT\tpredeterminer\t'all the kids'\n",
    "# POS\tpossessive ending\tparent\\'s\n",
    "# PRP\tpersonal pronoun\tI, he, she\n",
    "# PRP$\tpossessive pronoun\tmy, his, hers\n",
    "# RB\tadverb\tvery, silently,\n",
    "# RBR\tadverb, comparative\tbetter\n",
    "# RBS\tadverb, superlative\tbest\n",
    "# RP\tparticle\tgive up\n",
    "# TO\tto\tgo 'to' the store.\n",
    "# UH\tinterjection\terrrrrrrrm\n",
    "# VB\tverb, base form\ttake\n",
    "# VBD\tverb, past tense\ttook\n",
    "# VBG\tverb, gerund/present participle\ttaking\n",
    "# VBN\tverb, past participle\ttaken\n",
    "# VBP\tverb, sing. present, non-3d\ttake\n",
    "# VBZ\tverb, 3rd person sing. present\ttakes\n",
    "# WDT\twh-determiner\twhich\n",
    "# WP\twh-pronoun\twho, what\n",
    "# WP$\tpossessive wh-pronoun\twhose\n",
    "# WRB\twh-abverb\twhere, when\n",
    "\n",
    "# words - POS\n",
    "tags = pos_tag(words)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'DT')\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(tags)):\n",
    "    if tags[i][1] == 'DT':\n",
    "        print(tags[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i like this table in my room. it is made of bamboo wood. it is dark brown in color and has a polished finish. i have bought a chair with it to sit and study.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = 'i like this table in my room. it is made of bamboo wood. it is dark brown in color and has a polished finish. i have bought a chair with it to sit and study.'\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like this table in my room',\n",
       " 'it is made of bamboo wood',\n",
       " 'it is dark brown in color and has a polished finish',\n",
       " 'i have bought a chair with it to sit and study']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = corpus.split('.')\n",
    "\n",
    "sentences=[]\n",
    "for i in sen:\n",
    "    if i != '':\n",
    "        i = i.strip()\n",
    "        sentences.append(i)\n",
    "        \n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns: ('i', 'NN')\n",
      "Nouns: ('table', 'NN')\n",
      "Nouns: ('room', 'NN')\n",
      "Nouns: ('bamboo', 'NN')\n",
      "Nouns: ('wood', 'NN')\n",
      "Nouns: ('brown', 'NN')\n",
      "Nouns: ('color', 'NN')\n",
      "Nouns: ('finish', 'NN')\n",
      "Nouns: ('chair', 'NN')\n"
     ]
    }
   ],
   "source": [
    "for sen in sentences:\n",
    "    doc = sen.split(' ')\n",
    "    \n",
    "    tags = pos_tag(doc)\n",
    "    \n",
    "    for i in range(0, len(tags)):\n",
    "        if tags[i][1] == 'NN':\n",
    "            print('Nouns:',tags[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.6 Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to forget previous connections, to lose proxy related info\n",
    "\n",
    "session.trust_env = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for getting data of webpage\n",
    "\n",
    "response = session.get('http://cstags.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to display scrapped data\n",
    "# response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you're behind proxies then do this\n",
    "\n",
    "proxies = {'http':'http://192.168.1.23:179',\n",
    "           'https':'...'}\n",
    "\n",
    "response = session.get('http://cstags.pk', proxies = {'http':'http://192.168.1.23:179','https':'...'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.7 Text Segmentation and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'He owes me 22.50 dollars. Which is due by the next day.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He owes me 22', '50 dollars', ' Which is due by the next day', '']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem with splitting on the basis of . using split()\n",
    "sen = corpus.split('.')\n",
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He owes me 22.50 dollars.', 'Which is due by the next day.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'owes',\n",
       " 'me',\n",
       " '22.50',\n",
       " 'dollars',\n",
       " '.',\n",
       " 'Which',\n",
       " 'is',\n",
       " 'due',\n",
       " 'by',\n",
       " 'the',\n",
       " 'next',\n",
       " 'day',\n",
       " '.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'owes', 'me', '22.50', 'dollars', '.']\n",
      "['Which', 'is', 'due', 'by', 'the', 'next', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(corpus):\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from string import punctuation as punc\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('buttons_amazon_kindle.txt.data', encoding='Latin-1').read()\n",
    "corpus = corpus.lower()\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "\n",
    "for char in punc:\n",
    "    if char in corpus:\n",
    "        corpus = corpus.replace(char,'')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "\n",
    "words = corpus.split(' ')\n",
    "\n",
    "for w in words:\n",
    "    if w == '' or w == '\\n':\n",
    "        words.remove(w)\n",
    "        \n",
    "    if w in ENGLISH_STOP_WORDS:\n",
    "        words.remove(w)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS is not noun or verb\n",
    "\n",
    "tags = pos_tag(words)\n",
    "# tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tags = []\n",
    "\n",
    "for i in range(0, len(tags)):\n",
    "    if tags[i][1] != 'NN' and (tags[i][1][0] != 'V' and tags[i][1][1] != 'B' ):\n",
    "        n_tags.append(tags[i])\n",
    "        \n",
    "# n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
